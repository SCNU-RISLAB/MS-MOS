# MS-MOS: A Multi-Strip Model for Motion Segmentation

## ğŸ“–How to use
### ğŸ“¦pretrained model
Our pretrained model (validation with the IoU of **_77.6%_**) can be downloaded from [OneDrive](https://1drv.ms/f/c/9eeb7052c0f4734f/Ek9z9MBScOsggJ6y-QAAAAAB__QqT-TAtKKA-2luSsONjQ)
### ğŸ“šDataset 
Download SemanticKITTI dataset from [SemanticKITTI](http://www.semantic-kitti.org/dataset.html#download) (including **Velodyne point clouds**, **calibration data** , **pose data** and **label data**).
#### Preprocessing
After downloading the dataset, the residual maps as the input of the model during training need to be generated.
Run [auto_gen_residual_images.py](./MSMOS-v2/utils/auto_gen_residual_images.py) or [auto_gen_residual_images_mp.py](./MSMOS-v2/utils/auto_gen_residual_images_mp.py)(with multiprocess)
and check that the path is correct before running.

The structure of one of the folders in the entire dataset is as follows:
```
DATAROOT
â””â”€â”€ sequences
    â”œâ”€â”€ 00
    â”‚Â Â  â”œâ”€â”€ poses.txt
    â”‚Â Â  â”œâ”€â”€ calib.txt
    â”‚Â Â  â”œâ”€â”€ times.txt
    â”‚Â Â  â”œâ”€â”€ labels
    â”‚Â Â  â”œâ”€â”€ residual_images_1
    â”‚Â Â  â”œâ”€â”€ residual_images_2
    â”‚Â Â  â”œâ”€â”€ residual_images_3
    â”‚Â Â  â”œâ”€â”€ residual_images_4
    â”‚Â Â  â”œâ”€â”€ residual_images_5
    â”‚Â Â  â”œâ”€â”€ residual_images_6
    â”‚Â Â  â”œâ”€â”€ residual_images_7
    â”‚Â Â  â”œâ”€â”€ residual_images_8
    â”‚Â Â  â””â”€â”€ velodyne
   ...
```
### ğŸ’¾Environment
`Linux:`
Ubuntu 18.04, CUDA 11.1+Pytorch 1.7.1

Use conda to create the conda environment and activate it:
```shell
conda env create -f environment.yml
conda activate msmos
```

### ğŸ“Validation and Evaluation
Check the path in [valid.sh](./script/valid.sh) and [evaluate.sh](./script/evaluate.sh).

Then, run them to get the predicted results and IoU in the paper separately:
```shell
bash script/valid.sh
# evaluation after validation
bash script/evaluate.sh
```
You can also use our pre-trained model which has been provided above to validate its performance.

### ğŸ‘€Visualization
#### Single-frame visualization
Check the path in [visualize.sh](./script/visualize.sh), and run it to visualize the results in 2D and 3D:
```shell
bash script/visualize.sh
```
If -p is empty: only ground truth will be visualized.

If -p set the path of predictions: both ground truth and predictions will be visualized.
#### Get the sequences video
Check the path in [viz_seqVideo.py](./utils/viz_seqVideo.py), and run it to visualize the entire sequence in the form of a video.
